{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rental Scraper Notebook\n",
    "\n",
    "**Student ID:** 24204265  \n",
    "**Contact:** yash.pathania@ucdconnect.ie\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to scrape rental data for different quarters (Q1..Q4), parse specific fields (price, location, number of bedrooms, bathrooms, etc.), and store the scraped information in a CSV file.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Libraries:**  \n",
    "  We use libraries such as `requests` and `BeautifulSoup` for web scraping.\n",
    "\n",
    "- **Helper Functions:**  \n",
    "  Functions like `parse_price`, `parse_integer`, `parse_boolean`, and `parse_lease_length` assist in parsing the required fields.\n",
    "\n",
    "- **Scraping Function:**  \n",
    "  The main function, `scrape_rentals`, iterates through pages for each quarter until a 404 is encountered or until a preset limit is reached in test mode.\n",
    "\n",
    "- **Execution Modes:**  \n",
    "  - **Test Mode:** Scrapes only the first 5 pages per quarter for quicker testing.  \n",
    "  - **Full Mode:** Continues scraping until a 404 is returned for each quarter.\n",
    "\n",
    "- **Saving Data:**  \n",
    "  The scraped data is saved into a CSV file.\n",
    "\n",
    "## Key Notes\n",
    "\n",
    "- **Test Mode:**  \n",
    "  Designed for testing; only the first 5 pages per quarter are processed.\n",
    "\n",
    "- **Page Iteration:**  \n",
    "  Pages are scraped sequentially until a page returns a 404 error, indicating no more data for that quarter.\n",
    "\n",
    "- **Data Extraction:**  \n",
    "  The extraction process uses regex to parse data. While this method works for a basic website, it may not be fully robust.\n",
    "\n",
    "- **Comments:**\n",
    "  All comments are standard python function comments emphasiss on what cam in what came out the standarad python comment strcutre has been followed.\n",
    "\n",
    "## Contact\n",
    "\n",
    "For any clarifications or further details, please feel free to contact me at:  \n",
    "**yash.pathania@ucdconnect.ie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # To make calls\n",
    "from bs4 import BeautifulSoup # To parse the HTML\n",
    "import csv # To write to CSV\n",
    "import re # To use regular expressions ( extracting data from html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions imprtant are using regex to parse the unformatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_price(price_str):\n",
    "    \"\"\"\n",
    "    Extracts the numeric value from strings like '€ 1,920 per month'\n",
    "    and returns (float, 'EUR').\n",
    "    \"\"\"\n",
    "    currency = \"EUR\"\n",
    "    cleaned = price_str.replace(\"€\", \"\").replace(\"per month\", \"\").replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        numeric_price = float(cleaned)\n",
    "    except ValueError:\n",
    "        numeric_price = None\n",
    "    return numeric_price, currency\n",
    "\n",
    "def parse_integer(value_str):\n",
    "    \"\"\"\n",
    "    For '2 Bedrooms' or '1 Bathroom', return just the integer (2,1).\n",
    "    Returns None if no integer is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', value_str)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def parse_boolean(value_str):\n",
    "    \"\"\"\n",
    "    Interpret 'Yes'/'Y' (case-insensitive) as 'Yes'.\n",
    "    If '???' or missing, return '' (blank).\n",
    "    Otherwise -> 'No'.\n",
    "    \"\"\"\n",
    "    if not value_str:\n",
    "        return \"\"\n",
    "    val_str = value_str.strip().lower()\n",
    "    if val_str in (\"yes\", \"y\"):\n",
    "        return \"Yes\"\n",
    "    elif \"???\" in val_str:\n",
    "        return \"\"  # leave it blank\n",
    "    return \"No\"\n",
    "\n",
    "def parse_lease_length(value_str):\n",
    "    \"\"\"\n",
    "    Convert lease length to a numeric number of months:\n",
    "      '3 months' -> 3\n",
    "      '1 year'   -> 12\n",
    "      '???'      -> None\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', value_str)\n",
    "    if not match:\n",
    "        return None\n",
    "    num = int(match.group())\n",
    "    if 'year' in value_str.lower():\n",
    "        return num * 12\n",
    "    return num\n",
    "\n",
    "def parse_location(location_str):\n",
    "    \"\"\"\n",
    "    Splits the 'Location' field into (region, dublin_area).\n",
    "      e.g. \"Dublin City South - Dublin 16\" -> (\"Dublin City South\", \"16\")\n",
    "           \"North Co Dublin\"             -> (\"North Co Dublin\", \"\")\n",
    "    \"\"\"\n",
    "    parts = location_str.split('-')\n",
    "    region = parts[0].strip() if parts else location_str\n",
    "    dublin_area = \"\"\n",
    "    if len(parts) > 1:\n",
    "        # Look for \"Dublin 2\", \"Dublin 8\", etc. in the second part\n",
    "        match = re.search(r'Dublin\\s*(\\d+[A-Za-z0-9]*)', parts[1])\n",
    "        if match:\n",
    "            dublin_area = match.group(1)  # e.g. \"16\", \"2\", \"6W\", etc.\n",
    "    return region, dublin_area\n",
    "\n",
    "def fetch_page(url):\n",
    "    \"\"\"Fetches the content of the given URL.\"\"\"\n",
    "    return requests.get(url)\n",
    "\n",
    "def get_soup(response):\n",
    "    \"\"\"Converts a requests response to a BeautifulSoup object.\"\"\"\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def get_listings_container(soup):\n",
    "    \"\"\"\n",
    "    Finds the primary <ol> element that holds the listings.\n",
    "    Returns the <ol> or None if not found.\n",
    "    \"\"\"\n",
    "    return soup.find('ol')\n",
    "\n",
    "def extract_start_value(ol_element):\n",
    "    \"\"\"\n",
    "    Extracts the starting listing number from the <ol>'s 'start' attribute.\n",
    "    Defaults to 1 if not found or invalid.\n",
    "    \"\"\"\n",
    "    start_val = 1\n",
    "    if ol_element and 'start' in ol_element.attrs:\n",
    "        try:\n",
    "            start_val = int(ol_element['start'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return start_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Scraping Function ( breaks the page into tables and extractes data also does preprocessing to extract the write data using regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_listing(li_tag, listing_number, quarter, page_num):\n",
    "    \"\"\"\n",
    "    Processes a single <li> (one listing) and returns a dictionary of listing data.\n",
    "    \"\"\"\n",
    "    record_data = {\n",
    "        'ListingID': listing_number,\n",
    "        'Month': 'Unknown',\n",
    "        'Quarter': quarter,\n",
    "        'Page': page_num,\n",
    "        'Price': None,\n",
    "        'Currency': 'EUR',\n",
    "        'Property Type': '',\n",
    "        'Location': '',\n",
    "        'Bedrooms': None,\n",
    "        'Bathrooms': None,\n",
    "        'Parking': '',\n",
    "        'Garden': '',\n",
    "        'Lease Length (months)': None,\n",
    "        'Contact': ''\n",
    "    }\n",
    "    \n",
    "    # Extract month/year text (e.g. \"October 2024\")\n",
    "    month_span = li_tag.find('span', class_='record')\n",
    "    if month_span:\n",
    "        month_text = month_span.get_text(strip=True)\n",
    "        parts = month_text.split()\n",
    "        if parts:\n",
    "            record_data['Month'] = parts[0]  # e.g. \"October\" (we're ignoring the year here)\n",
    "    \n",
    "    # The table with class=\"rental\" has the property details\n",
    "    rental_table = li_tag.find('table', class_='rental')\n",
    "    if rental_table:\n",
    "        rows = rental_table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) != 2:\n",
    "                continue\n",
    "            field_name = cells[0].get_text(strip=True).replace(':', '')\n",
    "            field_value = cells[1].get_text(strip=True)\n",
    "            \n",
    "            if field_name == 'Price':\n",
    "                numeric_price, currency = parse_price(field_value)\n",
    "                record_data['Price'] = numeric_price\n",
    "                record_data['Currency'] = currency\n",
    "            elif field_name == 'Property Type':\n",
    "                record_data['Property Type'] = field_value\n",
    "            elif field_name == 'Location':\n",
    "                record_data['Location'] = field_value\n",
    "            elif field_name == 'Bedrooms':\n",
    "                record_data['Bedrooms'] = parse_integer(field_value)\n",
    "            elif field_name == 'Bathrooms':\n",
    "                record_data['Bathrooms'] = parse_integer(field_value)\n",
    "            elif field_name == 'Parking':\n",
    "                record_data['Parking'] = parse_boolean(field_value)\n",
    "            elif field_name == 'Garden':\n",
    "                record_data['Garden'] = parse_boolean(field_value)\n",
    "            elif field_name == 'Lease Length':\n",
    "                record_data['Lease Length (months)'] = parse_lease_length(field_value)\n",
    "            elif field_name == 'Contact':\n",
    "                record_data['Contact'] = field_value\n",
    "    \n",
    "    return record_data\n",
    "\n",
    "def process_page(base_url, quarter, page_num):\n",
    "    \"\"\"\n",
    "    Fetches and processes one page for a given quarter.\n",
    "    Returns (list_of_listings, count).\n",
    "    If a 404 occurs, returns (None, 0) to indicate no more pages.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}Q{quarter}-page{page_num:02d}.html\"\n",
    "    response = fetch_page(url)\n",
    "    \n",
    "    if response.status_code == 404:\n",
    "        print(f\"Got 404. Finished Q{quarter}.\")\n",
    "        return None, 0\n",
    "    \n",
    "    soup = get_soup(response)\n",
    "    listings_ol = get_listings_container(soup)\n",
    "    if not listings_ol:\n",
    "        print(\"No <ol> found on this page; skipping.\")\n",
    "        return [], 0\n",
    "    \n",
    "    start_val = extract_start_value(listings_ol)\n",
    "    li_tags = listings_ol.find_all('li', recursive=False)\n",
    "    \n",
    "    listings = []\n",
    "    for offset, li_tag in enumerate(li_tags):\n",
    "        listing_number = start_val + offset\n",
    "        record = process_listing(li_tag, listing_number, quarter, page_num)\n",
    "        listings.append(record)\n",
    "    \n",
    "    print(f\"Scraped Q{quarter}-Page {page_num}, found {len(li_tags)} listings.\")\n",
    "    return listings, len(li_tags)\n",
    "\n",
    "def scrape_rentals(base_url, test_mode=True):\n",
    "    \"\"\"\n",
    "    Scrapes rental data for Q1..Q4, stopping when a 404 is encountered.\n",
    "    If test_mode=True, only the first 5 pages per quarter are scraped.\n",
    "    Returns a list of dicts (all_records).\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    for quarter in range(1, 5):\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            listings, count = process_page(base_url, quarter, page_num)\n",
    "            if listings is None:  # 404 encountered\n",
    "                break\n",
    "            all_records.extend(listings)\n",
    "            page_num += 1\n",
    "            if test_mode and page_num > 5:\n",
    "                print(f\"Test mode active; stopping early for Q{quarter}.\")\n",
    "                break\n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Usefull Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(records):\n",
    "    \"\"\"\n",
    "    Enriches or transforms the final data before saving.\n",
    "    - Parse region & Dublin area from 'Location'\n",
    "    - Compute PricePerBedroom\n",
    "    - Add a 'LeaseCategory' column\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    new_records = []\n",
    "    for rec in records:\n",
    "        # Copy the original so we don't mutate in-place\n",
    "        r = dict(rec)\n",
    "        \n",
    "        # Parse region + area\n",
    "        region, dublin_area = parse_location(r[\"Location\"])\n",
    "        r[\"Region\"] = region\n",
    "        r[\"DublinArea\"] = dublin_area\n",
    "        \n",
    "        # Example: price per bedroom\n",
    "        if r[\"Price\"] and r[\"Bedrooms\"]:\n",
    "            r[\"PricePerBedroom\"] = round(r[\"Price\"] / r[\"Bedrooms\"], 2)\n",
    "        else:\n",
    "            r[\"PricePerBedroom\"] = None\n",
    "        \n",
    "        # Example: short vs long lease category\n",
    "        length_months = r[\"Lease Length (months)\"]\n",
    "        if length_months is not None:\n",
    "            r[\"LeaseCategory\"] = \"ShortTerm\" if length_months < 12 else \"LongTerm\"\n",
    "        else:\n",
    "            r[\"LeaseCategory\"] = \"Unknown\"\n",
    "        \n",
    "        new_records.append(r)\n",
    "    return new_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Names which can be extracted made modular for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldnames():\n",
    "    \"\"\"\n",
    "    Returns the list of field names for the rental data CSV.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        'ListingID',\n",
    "        'Month',\n",
    "        'Quarter',\n",
    "        'Page',\n",
    "        'Price',\n",
    "        'Currency',\n",
    "        'Property Type',\n",
    "        'Location',\n",
    "        'Bedrooms',\n",
    "        'Bathrooms',\n",
    "        'Parking',\n",
    "        'Garden',\n",
    "        'Lease Length (months)',\n",
    "        'Contact',\n",
    "        # New/enriched columns\n",
    "        'Region',\n",
    "        'DublinArea',\n",
    "        'PricePerBedroom',\n",
    "        'LeaseCategory'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital Scraper that starts scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(url, test_mode):\n",
    "    \"\"\"\n",
    "    Scrapes rental data from the provided URL.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL to scrape data from.\n",
    "        test_mode (bool): If True, only processes the first 5 pages per quarter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of rental records.\n",
    "    \"\"\"\n",
    "    print(f\"Starting scrape from {url}. Test mode is set to {test_mode}.\")\n",
    "    results = scrape_rentals(url, test_mode=test_mode)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Scrapped Data Into A Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv(csv_filename, results, fieldnames):\n",
    "    \"\"\"\n",
    "    Writes the rental data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        csv_filename (str): The name of the CSV file.\n",
    "        results (list): The list of rental records.\n",
    "        fieldnames (list): The CSV header fields.\n",
    "    \"\"\"\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in results:\n",
    "            writer.writerow(record)\n",
    "    print(f\"\\nDone! Wrote {len(results)} records to {csv_filename}\")\n",
    "\n",
    "def scrape_and_save(url, csv_filename, test_mode=False):\n",
    "    \"\"\"\n",
    "    Scrapes rental data from the given URL and writes the results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL to scrape data from.\n",
    "        csv_filename (str): The name of the CSV file to write.\n",
    "        test_mode (bool): If True, only processes the first 5 pages per quarter.\n",
    "    \"\"\"\n",
    "    results = scrape_data(url, test_mode)\n",
    "    results = augment_data(results)\n",
    "    \n",
    "    fieldnames = get_fieldnames()\n",
    "    write_results_to_csv(csv_filename, results, fieldnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using The Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape from http://mlg.ucd.ie/modules/python/assignment1/rental/. Test mode is set to False.\n",
      "Scraped Q1-Page 1, found 20 listings.\n",
      "Scraped Q1-Page 2, found 20 listings.\n",
      "Scraped Q1-Page 3, found 20 listings.\n",
      "Scraped Q1-Page 4, found 20 listings.\n",
      "Scraped Q1-Page 5, found 20 listings.\n",
      "Scraped Q1-Page 6, found 20 listings.\n",
      "Scraped Q1-Page 7, found 20 listings.\n",
      "Scraped Q1-Page 8, found 20 listings.\n",
      "Scraped Q1-Page 9, found 20 listings.\n",
      "Scraped Q1-Page 10, found 20 listings.\n",
      "Scraped Q1-Page 11, found 20 listings.\n",
      "Scraped Q1-Page 12, found 20 listings.\n",
      "Scraped Q1-Page 13, found 20 listings.\n",
      "Scraped Q1-Page 14, found 20 listings.\n",
      "Scraped Q1-Page 15, found 20 listings.\n",
      "Scraped Q1-Page 16, found 20 listings.\n",
      "Scraped Q1-Page 17, found 20 listings.\n",
      "Scraped Q1-Page 18, found 20 listings.\n",
      "Scraped Q1-Page 19, found 20 listings.\n",
      "Scraped Q1-Page 20, found 20 listings.\n",
      "Scraped Q1-Page 21, found 20 listings.\n",
      "Scraped Q1-Page 22, found 20 listings.\n",
      "Scraped Q1-Page 23, found 20 listings.\n",
      "Scraped Q1-Page 24, found 20 listings.\n",
      "Scraped Q1-Page 25, found 20 listings.\n",
      "Scraped Q1-Page 26, found 10 listings.\n",
      "Got 404. Finished Q1.\n",
      "Scraped Q2-Page 1, found 20 listings.\n",
      "Scraped Q2-Page 2, found 20 listings.\n",
      "Scraped Q2-Page 3, found 20 listings.\n",
      "Scraped Q2-Page 4, found 20 listings.\n",
      "Scraped Q2-Page 5, found 20 listings.\n",
      "Scraped Q2-Page 6, found 20 listings.\n",
      "Scraped Q2-Page 7, found 20 listings.\n",
      "Scraped Q2-Page 8, found 20 listings.\n",
      "Scraped Q2-Page 9, found 20 listings.\n",
      "Scraped Q2-Page 10, found 20 listings.\n",
      "Scraped Q2-Page 11, found 20 listings.\n",
      "Scraped Q2-Page 12, found 20 listings.\n",
      "Scraped Q2-Page 13, found 20 listings.\n",
      "Scraped Q2-Page 14, found 20 listings.\n",
      "Scraped Q2-Page 15, found 20 listings.\n",
      "Scraped Q2-Page 16, found 20 listings.\n",
      "Scraped Q2-Page 17, found 20 listings.\n",
      "Scraped Q2-Page 18, found 20 listings.\n",
      "Scraped Q2-Page 19, found 20 listings.\n",
      "Scraped Q2-Page 20, found 20 listings.\n",
      "Scraped Q2-Page 21, found 20 listings.\n",
      "Scraped Q2-Page 22, found 20 listings.\n",
      "Scraped Q2-Page 23, found 20 listings.\n",
      "Scraped Q2-Page 24, found 20 listings.\n",
      "Scraped Q2-Page 25, found 20 listings.\n",
      "Scraped Q2-Page 26, found 9 listings.\n",
      "Got 404. Finished Q2.\n",
      "Scraped Q3-Page 1, found 20 listings.\n",
      "Scraped Q3-Page 2, found 20 listings.\n",
      "Scraped Q3-Page 3, found 20 listings.\n",
      "Scraped Q3-Page 4, found 20 listings.\n",
      "Scraped Q3-Page 5, found 20 listings.\n",
      "Scraped Q3-Page 6, found 20 listings.\n",
      "Scraped Q3-Page 7, found 20 listings.\n",
      "Scraped Q3-Page 8, found 20 listings.\n",
      "Scraped Q3-Page 9, found 20 listings.\n",
      "Scraped Q3-Page 10, found 20 listings.\n",
      "Scraped Q3-Page 11, found 20 listings.\n",
      "Scraped Q3-Page 12, found 20 listings.\n",
      "Scraped Q3-Page 13, found 20 listings.\n",
      "Scraped Q3-Page 14, found 20 listings.\n",
      "Scraped Q3-Page 15, found 20 listings.\n",
      "Scraped Q3-Page 16, found 20 listings.\n",
      "Scraped Q3-Page 17, found 20 listings.\n",
      "Scraped Q3-Page 18, found 20 listings.\n",
      "Scraped Q3-Page 19, found 20 listings.\n",
      "Scraped Q3-Page 20, found 20 listings.\n",
      "Scraped Q3-Page 21, found 20 listings.\n",
      "Scraped Q3-Page 22, found 20 listings.\n",
      "Scraped Q3-Page 23, found 20 listings.\n",
      "Scraped Q3-Page 24, found 20 listings.\n",
      "Scraped Q3-Page 25, found 20 listings.\n",
      "Scraped Q3-Page 26, found 6 listings.\n",
      "Got 404. Finished Q3.\n",
      "Scraped Q4-Page 1, found 20 listings.\n",
      "Scraped Q4-Page 2, found 20 listings.\n",
      "Scraped Q4-Page 3, found 20 listings.\n",
      "Scraped Q4-Page 4, found 20 listings.\n",
      "Scraped Q4-Page 5, found 20 listings.\n",
      "Scraped Q4-Page 6, found 20 listings.\n",
      "Scraped Q4-Page 7, found 20 listings.\n",
      "Scraped Q4-Page 8, found 20 listings.\n",
      "Scraped Q4-Page 9, found 20 listings.\n",
      "Scraped Q4-Page 10, found 20 listings.\n",
      "Scraped Q4-Page 11, found 20 listings.\n",
      "Scraped Q4-Page 12, found 20 listings.\n",
      "Scraped Q4-Page 13, found 20 listings.\n",
      "Scraped Q4-Page 14, found 20 listings.\n",
      "Scraped Q4-Page 15, found 20 listings.\n",
      "Scraped Q4-Page 16, found 20 listings.\n",
      "Scraped Q4-Page 17, found 20 listings.\n",
      "Scraped Q4-Page 18, found 20 listings.\n",
      "Scraped Q4-Page 19, found 20 listings.\n",
      "Scraped Q4-Page 20, found 20 listings.\n",
      "Scraped Q4-Page 21, found 20 listings.\n",
      "Scraped Q4-Page 22, found 5 listings.\n",
      "Got 404. Finished Q4.\n",
      "\n",
      "Done! Wrote 1950 records to rentals.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"http://mlg.ucd.ie/modules/python/assignment1/rental/\"\n",
    "CSV_FILENAME = \"rentals.csv\"\n",
    "scrape_and_save(BASE_URL, CSV_FILENAME, test_mode=False) # Note: test_mode=False will scrape all pages and test mode true will scrape only 5 pages per quarter ( this was for testing purposes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
