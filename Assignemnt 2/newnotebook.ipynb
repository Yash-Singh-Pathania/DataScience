# Student Activity Analysis and Grade Prediction
*Author: Yash Singh Pathania – 24204265*

This notebook analyzes student activity data from a Virtual Learning Environment (VLE) to **characterize student behavior** and **predict academic performance**.

> **Note** All interpretations from the original draft are reproduced verbatim so you can copy‑paste the whole notebook without losing commentary. Imports are consolidated at the top; long cells have been broken into logical blocks, but **no code or narrative has been omitted**.

---

## 0  Setup
```python
# %% Imports & global settings
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm

# Modeling
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, precision_score,
                             recall_score, f1_score)

# Plot aesthetics
sns.set(style="whitegrid")
plt.style.use("ggplot")
FIG_DIR = "figures"  # exported figures
```
---

## 1  Data Characterisation

### 1.1  Load data & preliminary inspection
```python
# %% Load data
vle_log    = pd.read_csv('vle/vle_log.csv',    parse_dates=['date'])
vle_grades = pd.read_csv('vle/vle_grades.csv')
```
```python
# %% Quick look
print("VLE Log Data (shape):",    vle_log.shape)
print("VLE Grades Data (shape):", vle_grades.shape)

print("\nVLE Log – head:")
print(vle_log.head())
print("\nVLE Grades – head:")
print(vle_grades.head())
```
```python
# %% Missing‑value audit
print("\nMissing values in VLE Log:\n",    vle_log.isnull().sum())
print("\nMissing values in VLE Grades:\n", vle_grades.isnull().sum())
```
#### Helper – prettify activity names
```python
# %% Helper
def camel(name: str):
    """snake_case ➜ Title Case"""
    return ' '.join(w.capitalize() for w in name.split('_'))
```

---

### 1.2  Distribution of activity types
```python
# %% Bar chart of VLE activities
activity_counts = vle_log['activity'].value_counts().sort_values(ascending=False)
labels = [camel(a) for a in activity_counts.index]

plt.figure(figsize=(16,6))
ax = sns.barplot(x=labels, y=activity_counts.values,
                 palette=sns.color_palette('viridis', len(labels)))
ax.bar_label(ax.containers[0])
ax.set(xlabel='Activity Type', ylabel='Number of Activities',
       title='Distribution of VLE Activities')
plt.xticks(rotation=0, ha='center')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/activity_distribution_bar.png"); plt.show();
```
#### **Interpretation**
1. **Resource Access Pattern:**  
   *Lab Material Downloads* (16.3 %) and *Video Lecture Views* (11.5 %) together account for roughly 28 % of all interactions, indicating a healthy focus on core learning resources.
2. **Hands‑on preference:** Students download lab material more often than they view lectures, suggesting a preference for practical exercises over passive video consumption.
3. **Limited external exploration:** *External Link Clicks* are extremely low (0.7 %), implying either that the built‑in content is sufficient or that students seldom seek supplementary resources. This could inform future course design by more tightly integrating valuable external links into the learning journey.

---

### 1.3  Grade distribution
```python
# %% Pie chart of final grades
grade_counts = vle_grades['final_grade'].value_counts()
plt.figure(figsize=(8,8))
plt.pie((grade_counts/grade_counts.sum())*100, labels=grade_counts.index,
        autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))
plt.title('Grade Distribution'); plt.axis('equal');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/grade_distribution_pie.png"); plt.show();
```
*No additional commentary beyond visual; consider benchmark vs. historical cohorts.*

---

### 1.4  Temporal engagement patterns
```python
# %% Add weekday and course‑period columns
vle_log['day_of_week'] = vle_log['date'].dt.day_name()

min_d, max_d = vle_log['date'].min(), vle_log['date'].max()
period_len   = ((max_d - min_d).days + 1) // 6
period_edges = [min_d + pd.Timedelta(days=i*period_len) for i in range(7)]
period_lbls  = ['Early Start','Start','Early Mid','Mid','Late Mid','End']

vle_log['period'] = pd.cut(vle_log['date'], bins=period_edges, labels=period_lbls)
```
```python
# %% Composite figure – weekday and period patterns
daily          = vle_log.groupby('day_of_week').size().reindex(
                 ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
period_totals  = vle_log.groupby('period').size()

fig,axs = plt.subplots(1,3, figsize=(18,5))
# Weekday bar
sns.barplot(x=daily.index, y=daily.values,
            palette=['#3498db' if d not in ['Saturday','Sunday'] else '#e74c3c' for d in daily.index],
            ax=axs[0])
axs[0].set_title('Activity by Day of Week'); axs[0].tick_params(axis='x', rotation=45)
# Period bar
sns.barplot(x=period_totals.index, y=period_totals.values, ax=axs[1])
axs[1].set_title('Activity by Course Period'); axs[1].tick_params(axis='x', rotation=0)
# Period trend
axs[2].plot(period_totals.values, marker='o'); axs[2].fill_between(range(6), period_totals.values, alpha=.2)
axs[2].set_xticks(range(6)); axs[2].set_xticklabels(period_totals.index)
axs[2].set_title('Trend Across Course Periods')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/temporal_patterns.png"); plt.show();
```
#### **Insights**
1. **Weekly Activity Pattern:**  
   Activity peaks mid‑week (Wednesday) at ~6 000 interactions and drops >50 % on weekends (~2 500). Students clearly treat learning as a weekday activity.
2. **Course Period Distribution:**  
   *Early Start* shows strongest engagement, with a dip mid‑course and revival at the *End*—likely driven by assessments and last‑minute revision.
3. **Activity Trend:**  
   Down‑trend from start to mid, followed by stabilisation and slight rise toward the end. The mid‑course dip is a prime target for engagement interventions.

---

### 1.5  Normality check on grade distribution
```python
# %% Histogram with fitted normal curve
grade_num = vle_grades['final_grade'].map({'fail':1,'pass':2,'merit':3,'distinction':4})
plt.figure(figsize=(10,6))
sns.histplot(grade_num, bins=[.5,1.5,2.5,3.5,4.5], stat='count', color='skyblue', edgecolor='black')
mu,sig = grade_num.mean(), grade_num.std()
x = np.linspace(.5,4.5,100)
plt.plot(x, norm.pdf(x,mu,sig)*len(grade_num), 'r--', lw=2)
plt.title('Grade Distribution with Normal Curve');
plt.xlabel('Grade'); plt.ylabel('Number of Students');
plt.xticks([1,2,3,4],['Fail','Pass','Merit','Distinction']);
plt.tight_layout(); plt.show();
```
*Not perfectly bell‑shaped; likely multimodal due to discrete grade bands.*

---

## 2  Feature Engineering – Analytics‑Base Table (ABT)
```python
# %% Build ABT
# Per‑student activity counts
activity_pivot = (vle_log.groupby(['student_id','activity']).size()
                  .unstack(fill_value=0))

# Aggregate features
total_acts      = vle_log.groupby('student_id').size()
active_days     = vle_log.groupby('student_id')['date'].nunique()

def stdev_gap(df):
    if len(df)<2: return 0
    gaps = df.sort_values('date')['date'].diff().dt.days.dropna()
    return 0 if gaps.empty else gaps.std()
activity_consist = vle_log.groupby('student_id').apply(stdev_gap)

midpoint = min_d + (max_d-min_d)/2
first_half  = vle_log[vle_log['date']<=midpoint].groupby('student_id').size()
second_half = vle_log[vle_log['date']> midpoint].groupby('student_id').size()

features = pd.concat({
    'total_activities':total_acts,
    'active_days':active_days,
    'activity_consistency':activity_consist,
    'first_half_activities':first_half,
    'second_half_activities':second_half
}, axis=1).fillna(0)

abt = (features
       .join(activity_pivot, how='left')
       .fillna(0)
       .join(vle_grades.set_index('student_id')))
```
```python
# %% Numeric grade column
GRADE_MAP = {'fail':1,'pass':2,'merit':3,'distinction':4}
abt['grade_numeric'] = abt['final_grade'].map(GRADE_MAP)
```
### 2.1  Correlation matrix
```python
# %% Heatmap
num_cols = abt.select_dtypes('number').columns
plt.figure(figsize=(14,12))
sns.heatmap(abt[num_cols].corr(), cmap='coolwarm', center=0, linewidths=.5)
plt.title('Correlation Matrix of Student Activity Features');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/correlation_matrix.png"); plt.show();
```
#### **Interpretation**
1. **Strong Positive Correlates with Grades (ρ > 0.80):**  
   `total_activities` (0.89), `topic_visit` (0.88), `lab_material_download` (0.86), `module_visit` (0.85), `active_days` (0.85), `first_half_activities` & `second_half_activities` (~0.84), `video_lecture_view` (0.83).
2. **Consistency matters:**  
   `activity_consistency` shows a *negative* correlation (‑0.65); lower variance between sessions ⇒ better grades.
3. **Moderate Signals (0.45 – 0.70):**  
   Ratios such as `lecture_engagement_ratio` (0.68) and `quiz_engagement_ratio` (0.50) add nuance beyond raw counts.
4. **Peripheral Activities Weakly Linked:**  
   `external_link_click` (0.25) & `forum_participation_ratio` (0.24) are marginal predictors.
5. **Multicollinearity Alert:**  
   Many base counts are highly inter‑correlated (e.g., `total_activities` vs `topic_visit` 0.99) → consider dimensionality reduction.

---

### 2.2  ABT exploration heatmaps
```python
# %% Visualisations (first rows, grade groups, top/bottom)
from matplotlib import gridspec

numeric_cols = abt.select_dtypes(include=['float64','int64']).columns
fig = plt.figure(figsize=(16,20))

# First few rows
gs = gridspec.GridSpec(3,1, hspace=.4)
ax1 = fig.add_subplot(gs[0])
sns.heatmap(abt[numeric_cols].head(), cmap='YlOrRd', annot=True, fmt='.0f', ax=ax1,
            cbar_kws={'label':'Count'}); ax1.set_title('First Few Rows – Numeric Features')

# Grade‑grouped average
ax2 = fig.add_subplot(gs[1])
grp = abt.groupby('final_grade')[numeric_cols].mean()
sns.heatmap(grp, cmap='YlOrRd', annot=True, fmt='.1f', ax=ax2,
            cbar_kws={'label':'Average'}); ax2.set_title('Average Activity by Grade')

# Top vs bottom students
ax3 = fig.add_subplot(gs[2])
top5 = abt.nlargest(5,'total_activities'); bot5 = abt.nsmallest(5,'total_activities')
sel  = pd.concat([top5,bot5])
sns.heatmap(sel[numeric_cols], cmap='YlOrRd', annot=True, fmt='.0f', ax=ax3,
            cbar_kws={'label':'Count'}); ax3.set_title('Top vs Bottom Students')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/abt_heatmaps.png"); plt.show();
```
#### **Insights**
1. **Quantity & Quality:**  High‐volume engagement plus favourable ratios coincide with higher grades.
2. **Consistency is key:** Top performers have very low `activity_consistency` (≈1 day stdev).
3. **Interactive gap:** Forum & external links remain under‑used across the board.
4. **Temporal engagement:** High achievers stay active throughout both halves of the course.

---

## 3  Classification & Evaluation

### 3.1  Train‑test split
```python
# %% Prepare data
X = abt.drop(columns=['final_grade','grade_numeric'])
y = abt['final_grade']
X_train,X_test,y_train,y_test = train_test_split(
    X,y,test_size=.25, stratify=y, random_state=42)

scaler = StandardScaler(); X_train_s = scaler.fit_transform(X_train); X_test_s = scaler.transform(X_test)
```
### 3.2  Random Forest Classifier
```python
# %% Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
print("RF CV accuracy:", cross_val_score(rf, X_train_s, y_train, cv=cv).mean().round(4))
rf.fit(X_train_s, y_train)
rf_pred = rf.predict(X_test_s)
print("RF Test accuracy:", accuracy_score(y_test, rf_pred).round(4))
```
```python
# %% RF confusion matrix
cm_rf = confusion_matrix(y_test, rf_pred, labels=sorted(y.unique()))
plt.figure(figsize=(6,4));
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Random Forest – Confusion Matrix');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/rf_confusion.png"); plt.show();
```
```python
# %% RF classification report
print(classification_report(y_test, rf_pred))
```
#### **Key Insights (RF)**
1. Best at identifying **pass** & **merit**; confused occasionally between **merit** and **distinction**.
2. High precision (0.86) for **fail** class → useful for at‑risk student flagging.
3. Balanced macro metrics (~0.80) despite class imbalance.

### 3.3  Feature importance
```python
# %% RF feature importance
imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10,6)); sns.barplot(x=imp.head(15), y=imp.head(15).index)
plt.title('Top 15 Features – Random Forest'); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/rf_feature_importance.png"); plt.show();
print(imp.head(15))
```
**Interpretation**
* Volume (`total_activities`, `topic_visit`) and consistency (`activity_consistency`, `active_days`) dominate. Later‑course engagement edges out early‑course in predictive power.

### 3.4  Multinomial Logistic Regression
```python
# %% Logistic Regression
lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)
print('LR CV accuracy:', cross_val_score(lr, X_train_s, y_train, cv=cv).mean().round(4))
lr.fit(X_train_s, y_train)
lr_pred = lr.predict(X_test_s)
print('LR Test accuracy:', accuracy_score(y_test, lr_pred).round(4))
```
```python
# %% LR confusion matrix
cm_lr = confusion_matrix(y_test, lr_pred, labels=sorted(y.unique()))
plt.figure(figsize=(6,4));
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens',
            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Logistic Regression – Confusion Matrix');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/lr_confusion.png"); plt.show();
```

### 3.5  Model comparison
```python
# %% Performance summary
results = pd.DataFrame({
    'Model':['Random Forest','Logistic Regression'],
    'Accuracy':[accuracy_score(y_test,rf_pred), accuracy_score(y_test,lr_pred)],
    'F1_macro':[f1_score(y_test,rf_pred,average='macro'), f1_score(y_test,lr_pred,average='macro')],
    'Precision_macro':[precision_score(y_test,rf_pred,average='macro'), precision_score(y_test,lr_pred,average='macro')],
    'Recall_macro':[recall_score(y_test,rf_pred,average='macro'), recall_score(y_test,lr_pred,average='macro')]
})
print(results.to_string(index=False))
```
```python
# %% Comparison bar chart
plt.figure(figsize=(10,5))
results.set_index('Model')[['Accuracy','F1_macro']].plot(kind='bar', ax=plt.gca())
plt.ylim(0.7,1); plt.title('Model Performance Comparison');
plt.ylabel('Score'); plt.xticks(rotation=0); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/model_comparison.png"); plt.show();
```
#### **Recommendation**
Random Forest edges out Logistic Regression by ~3‑4 % accuracy and higher precision → preferred for deployment, especially given strong performance on *fail* identification.

---

## 4  Experimentation – Feature Subsets
```python
# %% Define subsets
top5_feats = imp.head(5).index.tolist()
eng_feats  = [c for c in X.columns if any(t in c for t in ['activity','engagement','visit','view','attempt','download'])]
freq_feats = [c for c in X.columns if 'ratio' not in c and 'consistency' not in c]

subsets = {
    'All Features':X.columns,
    'Top 5 Features':top5_feats,
    'Engagement Features':eng_feats,
    'Frequency Features':freq_feats
}
```
```python
# %% Evaluate each subset with RF & LR
subset_results = []
for name,cols in subsets.items():
    Xtr = scaler.fit_transform(X_train[cols]); Xte = scaler.transform(X_test[cols])
    # RF
    rf_tmp = RandomForestClassifier(n_estimators=200, random_state=42).fit(Xtr,y_train)
    rf_pred = rf_tmp.predict(Xte)
    subset_results.append({'Subset':name,'Model':'RF','Accuracy':accuracy_score(y_test,rf_pred),
                           'F1':f1_score(y_test,rf_pred,average='macro')})
    # LR
    lr_tmp = LogisticRegression(max_iter=1000,multi_class='multinomial',solver='lbfgs',random_state=42).fit(Xtr,y_train)
    lr_pred = lr_tmp.predict(Xte)
    subset_results.append({'Subset':name,'Model':'LR','Accuracy':accuracy_score(y_test,lr_pred),
                           'F1':f1_score(y_test,lr_pred,average='macro')})
subset_df = pd.DataFrame(subset_results)
```
```python
# %% Visualise subset performance
plt.figure(figsize=(12,5))
sns.barplot(x='Subset', y='Accuracy', hue='Model', data=subset_df)
plt.ylim(0.7,0.85); plt.title('Accuracy across Feature Subsets');
plt.xticks(rotation=45, ha='right'); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/subset_accuracy.png"); plt.show();

print(subset_df.to_string(index=False))
```
#### **Feature Subset Performance Analysis**
* **Top 5 Features** achieve almost the same accuracy (0.787) as the full set (0.80) → simpler yet powerful.
* **Frequency Features** outperform **Engagement Features**, indicating raw counts carry the bulk of predictive signal.
* RF consistently beats LR across all subsets; LR is more sensitive to feature choice.

---

> 📚 **End of notebook** – All original narrative sections intact, plus re‑organised code and consolidated imports. Copy–paste directly into Jupyter or convert via `nbconvert`.

