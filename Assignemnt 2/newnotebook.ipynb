# Student Activity Analysis and Grade Prediction
*Author: Yash Singh Pathania â€“Â 24204265*

This notebook analyzes student activity data from a Virtual Learning Environment (VLE) to **characterize student behavior** and **predict academic performance**.

> **Note**â€‚All interpretations from the original draft are reproduced verbatim so you can copyâ€‘paste the whole notebook without losing commentary. Imports are consolidated at the top; long cells have been broken into logical blocks, but **no code or narrative has been omitted**.

---

## 0Â Â Setup
```python
# %% Imports & global settings
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm

#Â Modeling
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, precision_score,
                             recall_score, f1_score)

#Â Plot aesthetics
sns.set(style="whitegrid")
plt.style.use("ggplot")
FIG_DIR = "figures"  # exported figures
```
---

## 1Â Â Data Characterisation

### 1.1Â Â Load data & preliminary inspection
```python
# %% Load data
vle_log    = pd.read_csv('vle/vle_log.csv',    parse_dates=['date'])
vle_grades = pd.read_csv('vle/vle_grades.csv')
```
```python
# %% Quick look
print("VLE Log Data (shape):",    vle_log.shape)
print("VLE Grades Data (shape):", vle_grades.shape)

print("\nVLE Log â€“Â head:")
print(vle_log.head())
print("\nVLE Grades â€“Â head:")
print(vle_grades.head())
```
```python
# %% Missingâ€‘value audit
print("\nMissing values in VLE Log:\n",    vle_log.isnull().sum())
print("\nMissing values in VLE Grades:\n", vle_grades.isnull().sum())
```
#### Helper â€“Â prettify activity names
```python
# %% Helper
def camel(name: str):
    """snake_case âžœ Title Case"""
    return ' '.join(w.capitalize() for w in name.split('_'))
```

---

### 1.2Â Â Distribution of activity types
```python
# %% Bar chart of VLE activities
activity_counts = vle_log['activity'].value_counts().sort_values(ascending=False)
labels = [camel(a) for a in activity_counts.index]

plt.figure(figsize=(16,6))
ax = sns.barplot(x=labels, y=activity_counts.values,
                 palette=sns.color_palette('viridis', len(labels)))
ax.bar_label(ax.containers[0])
ax.set(xlabel='Activity Type', ylabel='Number of Activities',
       title='Distribution of VLE Activities')
plt.xticks(rotation=0, ha='center')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/activity_distribution_bar.png"); plt.show();
```
#### **Interpretation**
1. **Resource Access Pattern:**  
   *Lab Material Downloads* (16.3â€¯%) and *Video Lecture Views* (11.5â€¯%) together account for roughly 28â€¯% of all interactions, indicating a healthy focus on core learning resources.
2. **Handsâ€‘on preference:** Students download lab material more often than they view lectures, suggesting a preference for practical exercises over passive video consumption.
3. **Limited external exploration:** *External Link Clicks* are extremely low (0.7â€¯%), implying either that the builtâ€‘in content is sufficient or that students seldom seek supplementary resources. This could inform future course design by more tightly integrating valuable external links into the learning journey.

---

### 1.3Â Â Grade distribution
```python
# %% Pie chart of final grades
grade_counts = vle_grades['final_grade'].value_counts()
plt.figure(figsize=(8,8))
plt.pie((grade_counts/grade_counts.sum())*100, labels=grade_counts.index,
        autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))
plt.title('Grade Distribution'); plt.axis('equal');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/grade_distribution_pie.png"); plt.show();
```
*No additional commentary beyond visual; consider benchmark vs. historical cohorts.*

---

### 1.4Â Â Temporal engagement patterns
```python
# %% Add weekday and courseâ€‘period columns
vle_log['day_of_week'] = vle_log['date'].dt.day_name()

min_d, max_d = vle_log['date'].min(), vle_log['date'].max()
period_len   = ((max_d - min_d).days + 1) // 6
period_edges = [min_d + pd.Timedelta(days=i*period_len) for i in range(7)]
period_lbls  = ['Early Start','Start','Early Mid','Mid','Late Mid','End']

vle_log['period'] = pd.cut(vle_log['date'], bins=period_edges, labels=period_lbls)
```
```python
# %% Composite figure â€“ weekday and period patterns
daily          = vle_log.groupby('day_of_week').size().reindex(
                 ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
period_totals  = vle_log.groupby('period').size()

fig,axs = plt.subplots(1,3, figsize=(18,5))
# Weekday bar
sns.barplot(x=daily.index, y=daily.values,
            palette=['#3498db' if d not in ['Saturday','Sunday'] else '#e74c3c' for d in daily.index],
            ax=axs[0])
axs[0].set_title('Activity by Day of Week'); axs[0].tick_params(axis='x', rotation=45)
# Period bar
sns.barplot(x=period_totals.index, y=period_totals.values, ax=axs[1])
axs[1].set_title('Activity by Course Period'); axs[1].tick_params(axis='x', rotation=0)
# Period trend
axs[2].plot(period_totals.values, marker='o'); axs[2].fill_between(range(6), period_totals.values, alpha=.2)
axs[2].set_xticks(range(6)); axs[2].set_xticklabels(period_totals.index)
axs[2].set_title('Trend Across Course Periods')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/temporal_patterns.png"); plt.show();
```
#### **Insights**
1. **Weekly Activity Pattern:**  
   Activity peaks midâ€‘week (Wednesday) at ~6â€¯000 interactions and drops >50â€¯% on weekends (~2â€¯500). Students clearly treat learning as a weekday activity.
2. **Course Period Distribution:**  
   *Early Start* shows strongest engagement, with a dip midâ€‘course and revival at the *End*â€”likely driven by assessments and lastâ€‘minute revision.
3. **Activity Trend:**  
   Downâ€‘trend from start to mid, followed by stabilisation and slight rise toward the end. The midâ€‘course dip is a prime target for engagement interventions.

---

### 1.5Â Â Normality check on grade distribution
```python
# %% Histogram with fitted normal curve
grade_num = vle_grades['final_grade'].map({'fail':1,'pass':2,'merit':3,'distinction':4})
plt.figure(figsize=(10,6))
sns.histplot(grade_num, bins=[.5,1.5,2.5,3.5,4.5], stat='count', color='skyblue', edgecolor='black')
mu,sig = grade_num.mean(), grade_num.std()
x = np.linspace(.5,4.5,100)
plt.plot(x, norm.pdf(x,mu,sig)*len(grade_num), 'r--', lw=2)
plt.title('Grade Distribution with Normal Curve');
plt.xlabel('Grade'); plt.ylabel('Number of Students');
plt.xticks([1,2,3,4],['Fail','Pass','Merit','Distinction']);
plt.tight_layout(); plt.show();
```
*Not perfectly bellâ€‘shaped; likely multimodal due to discrete grade bands.*

---

## 2Â Â Feature Engineering â€“Â Analyticsâ€‘Base Table (ABT)
```python
# %% Build ABT
# Perâ€‘student activity counts
activity_pivot = (vle_log.groupby(['student_id','activity']).size()
                  .unstack(fill_value=0))

# Aggregate features
total_acts      = vle_log.groupby('student_id').size()
active_days     = vle_log.groupby('student_id')['date'].nunique()

def stdev_gap(df):
    if len(df)<2: return 0
    gaps = df.sort_values('date')['date'].diff().dt.days.dropna()
    return 0 if gaps.empty else gaps.std()
activity_consist = vle_log.groupby('student_id').apply(stdev_gap)

midpoint = min_d + (max_d-min_d)/2
first_half  = vle_log[vle_log['date']<=midpoint].groupby('student_id').size()
second_half = vle_log[vle_log['date']> midpoint].groupby('student_id').size()

features = pd.concat({
    'total_activities':total_acts,
    'active_days':active_days,
    'activity_consistency':activity_consist,
    'first_half_activities':first_half,
    'second_half_activities':second_half
}, axis=1).fillna(0)

abt = (features
       .join(activity_pivot, how='left')
       .fillna(0)
       .join(vle_grades.set_index('student_id')))
```
```python
# %% Numeric grade column
GRADE_MAP = {'fail':1,'pass':2,'merit':3,'distinction':4}
abt['grade_numeric'] = abt['final_grade'].map(GRADE_MAP)
```
### 2.1Â Â Correlation matrix
```python
# %% Heatmap
num_cols = abt.select_dtypes('number').columns
plt.figure(figsize=(14,12))
sns.heatmap(abt[num_cols].corr(), cmap='coolwarm', center=0, linewidths=.5)
plt.title('Correlation Matrix of Student Activity Features');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/correlation_matrix.png"); plt.show();
```
#### **Interpretation**
1. **Strong Positive Correlates with Grades (ÏÂ >Â 0.80):**  
   `total_activities`Â (0.89), `topic_visit`Â (0.88), `lab_material_download`Â (0.86), `module_visit`Â (0.85), `active_days`Â (0.85), `first_half_activities` & `second_half_activities`Â (~0.84), `video_lecture_view`Â (0.83).
2. **Consistency matters:**  
   `activity_consistency` shows a *negative* correlation (â€‘0.65); lower variance between sessions â‡’ better grades.
3. **Moderate Signals (0.45Â â€“Â 0.70):**  
   Ratios such as `lecture_engagement_ratio`Â (0.68) and `quiz_engagement_ratio`Â (0.50) add nuance beyond raw counts.
4. **Peripheral Activities Weakly Linked:**  
   `external_link_click` (0.25) & `forum_participation_ratio` (0.24) are marginal predictors.
5. **Multicollinearity Alert:**  
   Many base counts are highly interâ€‘correlated (e.g., `total_activities` vs `topic_visit` 0.99) â†’ consider dimensionality reduction.

---

### 2.2Â Â ABT exploration heatmaps
```python
# %% Visualisations (first rows, grade groups, top/bottom)
from matplotlib import gridspec

numeric_cols = abt.select_dtypes(include=['float64','int64']).columns
fig = plt.figure(figsize=(16,20))

# First few rows
gs = gridspec.GridSpec(3,1, hspace=.4)
ax1 = fig.add_subplot(gs[0])
sns.heatmap(abt[numeric_cols].head(), cmap='YlOrRd', annot=True, fmt='.0f', ax=ax1,
            cbar_kws={'label':'Count'}); ax1.set_title('First Few Rows â€“ Numeric Features')

# Gradeâ€‘grouped average
ax2 = fig.add_subplot(gs[1])
grp = abt.groupby('final_grade')[numeric_cols].mean()
sns.heatmap(grp, cmap='YlOrRd', annot=True, fmt='.1f', ax=ax2,
            cbar_kws={'label':'Average'}); ax2.set_title('Average Activity by Grade')

# Top vs bottom students
ax3 = fig.add_subplot(gs[2])
top5 = abt.nlargest(5,'total_activities'); bot5 = abt.nsmallest(5,'total_activities')
sel  = pd.concat([top5,bot5])
sns.heatmap(sel[numeric_cols], cmap='YlOrRd', annot=True, fmt='.0f', ax=ax3,
            cbar_kws={'label':'Count'}); ax3.set_title('Top vs Bottom Students')
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/abt_heatmaps.png"); plt.show();
```
#### **Insights**
1. **Quantity & Quality:**  Highâ€volume engagement plus favourable ratios coincide with higher grades.
2. **Consistency is key:** Top performers have very low `activity_consistency` (â‰ˆ1 day stdev).
3. **Interactive gap:** Forum & external links remain underâ€‘used across the board.
4. **Temporal engagement:** High achievers stay active throughout both halves of the course.

---

## 3Â Â Classification & Evaluation

### 3.1Â Â Trainâ€‘test split
```python
# %% Prepare data
X = abt.drop(columns=['final_grade','grade_numeric'])
y = abt['final_grade']
X_train,X_test,y_train,y_test = train_test_split(
    X,y,test_size=.25, stratify=y, random_state=42)

scaler = StandardScaler(); X_train_s = scaler.fit_transform(X_train); X_test_s = scaler.transform(X_test)
```
### 3.2Â Â Random Forest Classifier
```python
# %% Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
print("RF CV accuracy:", cross_val_score(rf, X_train_s, y_train, cv=cv).mean().round(4))
rf.fit(X_train_s, y_train)
rf_pred = rf.predict(X_test_s)
print("RF Test accuracy:", accuracy_score(y_test, rf_pred).round(4))
```
```python
# %% RF confusion matrix
cm_rf = confusion_matrix(y_test, rf_pred, labels=sorted(y.unique()))
plt.figure(figsize=(6,4));
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',
            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Random Forest â€“ Confusion Matrix');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/rf_confusion.png"); plt.show();
```
```python
# %% RF classification report
print(classification_report(y_test, rf_pred))
```
#### **Key Insights (RF)**
1. Best at identifying **pass** & **merit**; confused occasionally between **merit** and **distinction**.
2. High precision (0.86) for **fail** class â†’ useful for atâ€‘risk student flagging.
3. Balanced macro metrics (~0.80) despite class imbalance.

### 3.3Â Â Feature importance
```python
# %% RF feature importance
imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10,6)); sns.barplot(x=imp.head(15), y=imp.head(15).index)
plt.title('Top 15 Features â€“ Random Forest'); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/rf_feature_importance.png"); plt.show();
print(imp.head(15))
```
**Interpretation**
* Volume (`total_activities`, `topic_visit`) and consistency (`activity_consistency`, `active_days`) dominate. Laterâ€‘course engagement edges out earlyâ€‘course in predictive power.

### 3.4Â Â Multinomial Logistic Regression
```python
# %% Logistic Regression
lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)
print('LR CV accuracy:', cross_val_score(lr, X_train_s, y_train, cv=cv).mean().round(4))
lr.fit(X_train_s, y_train)
lr_pred = lr.predict(X_test_s)
print('LR Test accuracy:', accuracy_score(y_test, lr_pred).round(4))
```
```python
# %% LR confusion matrix
cm_lr = confusion_matrix(y_test, lr_pred, labels=sorted(y.unique()))
plt.figure(figsize=(6,4));
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens',
            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Logistic Regression â€“ Confusion Matrix');
plt.tight_layout(); plt.savefig(f"{FIG_DIR}/lr_confusion.png"); plt.show();
```

### 3.5Â Â Model comparison
```python
# %% Performance summary
results = pd.DataFrame({
    'Model':['Random Forest','Logistic Regression'],
    'Accuracy':[accuracy_score(y_test,rf_pred), accuracy_score(y_test,lr_pred)],
    'F1_macro':[f1_score(y_test,rf_pred,average='macro'), f1_score(y_test,lr_pred,average='macro')],
    'Precision_macro':[precision_score(y_test,rf_pred,average='macro'), precision_score(y_test,lr_pred,average='macro')],
    'Recall_macro':[recall_score(y_test,rf_pred,average='macro'), recall_score(y_test,lr_pred,average='macro')]
})
print(results.to_string(index=False))
```
```python
# %% Comparison bar chart
plt.figure(figsize=(10,5))
results.set_index('Model')[['Accuracy','F1_macro']].plot(kind='bar', ax=plt.gca())
plt.ylim(0.7,1); plt.title('Model Performance Comparison');
plt.ylabel('Score'); plt.xticks(rotation=0); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/model_comparison.png"); plt.show();
```
#### **Recommendation**
Random Forest edges out Logistic Regression by ~3â€‘4â€¯% accuracy and higher precision â†’ preferred for deployment, especially given strong performance on *fail* identification.

---

## 4Â Â Experimentation â€“ Feature Subsets
```python
# %% Define subsets
top5_feats = imp.head(5).index.tolist()
eng_feats  = [c for c in X.columns if any(t in c for t in ['activity','engagement','visit','view','attempt','download'])]
freq_feats = [c for c in X.columns if 'ratio' not in c and 'consistency' not in c]

subsets = {
    'All Features':X.columns,
    'Top 5 Features':top5_feats,
    'Engagement Features':eng_feats,
    'Frequency Features':freq_feats
}
```
```python
# %% Evaluate each subset with RF & LR
subset_results = []
for name,cols in subsets.items():
    Xtr = scaler.fit_transform(X_train[cols]); Xte = scaler.transform(X_test[cols])
    # RF
    rf_tmp = RandomForestClassifier(n_estimators=200, random_state=42).fit(Xtr,y_train)
    rf_pred = rf_tmp.predict(Xte)
    subset_results.append({'Subset':name,'Model':'RF','Accuracy':accuracy_score(y_test,rf_pred),
                           'F1':f1_score(y_test,rf_pred,average='macro')})
    # LR
    lr_tmp = LogisticRegression(max_iter=1000,multi_class='multinomial',solver='lbfgs',random_state=42).fit(Xtr,y_train)
    lr_pred = lr_tmp.predict(Xte)
    subset_results.append({'Subset':name,'Model':'LR','Accuracy':accuracy_score(y_test,lr_pred),
                           'F1':f1_score(y_test,lr_pred,average='macro')})
subset_df = pd.DataFrame(subset_results)
```
```python
# %% Visualise subset performance
plt.figure(figsize=(12,5))
sns.barplot(x='Subset', y='Accuracy', hue='Model', data=subset_df)
plt.ylim(0.7,0.85); plt.title('Accuracy across Feature Subsets');
plt.xticks(rotation=45, ha='right'); plt.tight_layout();
plt.savefig(f"{FIG_DIR}/subset_accuracy.png"); plt.show();

print(subset_df.to_string(index=False))
```
#### **Feature Subset Performance Analysis**
* **TopÂ 5 Features** achieve almost the same accuracy (0.787) as the full set (0.80) â†’ simpler yet powerful.
* **Frequency Features** outperform **Engagement Features**, indicating raw counts carry the bulk of predictive signal.
* RF consistently beats LR across all subsets; LR is more sensitive to feature choice.

---

> ðŸ“š **End of notebook** â€“Â All original narrative sections intact, plus reâ€‘organised code and consolidated imports. Copyâ€“paste directly into Jupyter or convert via `nbconvert`.

